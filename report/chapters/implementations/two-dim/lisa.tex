Spatial data and query processing have become ubiquitous due to the proliferation of location-based services such as digital mapping, location-based social networking, and geo-targeted advertising. Motivated by the performance benefits of learned indices for one-dimensional data, this section explores the application of learned index for spatial data. The key idea in LISA \cite{li2020lisa} is to map spatial data into one-dimensional data through several steps and apply machine learning techniques to generate a learned index for the one-dimensional data.

\subsubsection{Motivation}

In the section \ref{sssec:RMI}, we described a recursive model index (RMI) that consists of a number of machine learning models staged into a hierarchy to enable synthesis of specialised index structures, termed learned indexes. Provided with a search key $x$, RMI predicts the position of $x$'s data with some error bound, by learning the CDF over the key search space. However, the idea of RMI is not applicable in the context of spatial data as spatial data invalidates the assumption required by RMI that the data is sorted by key and that any imprecision can be easily corrected by a localised search. Although it is possible to learn multi-dimensional CDFs, such CDFs will result in searching local regions qualified on one dimension but not all dimensions.
\comm{  
\begin{figure*}[t]
    \centering
    \includegraphics[width=0.6\textwidth]{graphs/LISA_Key_distribution.png}
    \caption{Key Distribution in 2 dimensional case:Idea of learned indexes is not applicable in the context of spatial data as data is not sorted by key. Learning multidimensional CDFs will result in searching local regions qualified on one dimension but not all dimensions.}
    \label{fig:Key_Distribution}
\end{figure*}
}

For the one-dimensional data, we can learn the CDF by using a recursive model as shown in previous section. However, when the data is two-dimensional, the learned CDF (the marginal CDF) through the recursive model cannot be applied directly to predict the position of the key. Formally, we could learn the marginal CDF for each dimension by using the recursive model, i.e. $F(X)$ and $F(Y)$. However, to predict the position of a $2$-dimensional key, we need to joint CDF $F(X,Y)$, which cannot be induced from the marginal CDFs. We show an example as below to illustrate this limitation.

\begin{mscexample}
	Assume that $X$ and $Y$ are distributed as shown in Fig \ref{fig:2d_limitation_rmi}. In this example, we have $F(x\leq A)=\frac{1}{3}$, which means that the point $A$ should be assigned into the first third pages. With learned indexes in one-dimensional, then there comes the problem below:
	
	\begin{enumerate}
		\item There will be duplicate keys. In this example, if we only consider the $X$ axis, we will get an array $[0.7, 0.7, 1.5]$ which contains duplicate keys. 
		\item If we remove the duplicate keys, then $F(x\leq A)=\frac{1}{2}$, which is not what we expect.
		\item If we do not remove the duplicate keys, then $F(x\leq A)=F(x\leq B)$, which is still not we expect.
	\end{enumerate}
	

		\centering
		\includegraphics[scale=0.5]{graphs/implementation/2d/2d_rmi_limitation}
		\captionof{figure}{An example demonstrating the limitations of one-dimensional learned index in two-dimensional data. In this graph we have $F(x\leq A)=\frac{1}{3}$ but with learned index in one-dimensional, we cannot learn such joint CDF.}
		\label{fig:2d_limitation_rmi}

	
\end{mscexample}

LISA builds on the work from Tim Kraska \cite{kraska2018case} and solves this problem by mapping the 2-D key space into a 1-D sorted array and use linear regression functions to learn the cdf. It partitions the 2-D key space into a series of grid cells based on the data distribution and builds a partially monotonic function to map the data from $\mathbb{R}^d$ into $\mathbb{R}$, in our case, we have $d=2$. We call this function as \textit{Mapping Function}.

\subsubsection{Definitions}

This section presents the definition

\begin{enumerate}
	\item \textbf{Key}. A key k is a unique identifier for a data record with $k = (x_{0}, x_{1}) \in \mathbb{R}^{2}$. 
    
	\item \textbf{Cell}. A grid cell is a rectangle whose lower and upper corners are points $(l_0, l_1)$ and  $(u_0,u_1)$, i.e.,  cell = $(l_{0},u_{0}) \times [l_{1},u_{1})$.
	
	\item \textbf{Mapping Function}. A mapping function $\mathcal{M}$ is a function on the domain $\mathbb{R}^2$ to the non-negative range, i.e $\mathcal{M}:[0,X_{0}]\times [0,X_{1}]\to [0,+\infty)$ such that $M(x_0,x_{1}) \leq \mathcal{M}(y_{0},y_1)$ when $x_0 \leq y_0$ and $x_1 \leq y_1$.
\end{enumerate}

\subsection{Baseline Method}  

We can extend the learned index method for range queries on spatial data by using a mapping function. This baseline method works as follows. We first sort all keys according to their mapped values and divide the mapped values into some cells such that each cell contains the same number of keys (except the last one). If a point $(x,y)$â€™s mapped value is larger than those of the keys stored in the first $i$ cells, i.e. $\mathcal{M}(x,y) > \sup \bigcup\limits_{j=0}^{i-1} M(C_{j})$, we store $(x,y)$ in the $(i+1)$th cell. 

For a range query, represented by the query rectangle $qr = [l_{0},u_{0}) \times [l_{1},u_{1})$, We only need to predict the indices of $(l_{0}, l_{1})$ and $(u_{0},u_{1})$ namely $i_{1}$ and $i_{2}$ respectively. Then we scan the keys in $i_{2}-i_{1}+1$ cells, and find those keys that fall in the query rectangle qr. 

\begin{mscexample}
Conider the example shown in Fig. \ref{fig:BaseLine_Method}. . During prediction, we need to find out the cell to which our query point belongs (the $2^{nd}$ cell in our example). .

	\begin{enumerate}
		\item The 2-D key space is divided into 3 cells using the mapping function $\mathcal{M}((x,y))= x+y$. Each section in Fig. \ref{fig:BaseLine_Method} represents one cell. 
		\item The query point is represented by the blue rectangle. It consists of only $1$ key and falls inside the second cell. 
		\item Identify the cell to which the query rectangle belongs by doing a binary search based on query point mapped value. 
		\item Once the cell $2$ is identified, we need to compare the 2-D key value of the query point, against all the possible keys in that cell 2 until a match is found. This can results in maximum of $8$ irrelevant points being accessed for the point query.
	\end{enumerate}
\end{mscexample}

\begin{figure*}[t]
    \centering
    \includegraphics[width=1.1\textwidth]{graphs/implementation/Baseline_limitation_chen_limitation.pdf}
    \caption{LISA Baseline Method. In this figure, a) Key space is divided into 3 cells with equal number of keys. b) To search for a query, we first need to find out the cell which contain the query point. c) Once the query point is found, we need to compare the query point 2 dimensional key value with all the keys in the cell until a match is found}
    \label{fig:BaseLine_Method}
\end{figure*}
\subsubsection{Training}

The training dataset for the baseline model can be notated as $(\boldsymbol{X}, Y)$ with entries notated as $(\boldsymbol{x},y)$. $\boldsymbol{X}$ represents the two dimensional key coordinates, and $Y$ represents the corresponding data item. 

\begin{algorithm}[H]
    \SetAlgoLined
    \SetKwInOut{Input}{input}
    \SetKwInOut{Output}{Output}
    \Input{$N$: number\_of\_cells; trainset=[$(x,y);x \in \mathbb{R}^{2};y \in \mathbb{R}$]}
    \Output{cell: Array containing cells' metadata}
     \For{$i\gets0$ \KwTo $len($x$)$}{
            	\texttt{$x$[$i$].mapped\_value = $x$[$i$][0]+$x$[$i$][1]} \\
            
     }
     
     \texttt{$K$ = len$(x) \slash N$} // keys per cell \\
     \texttt{$x$ = $x$[argsort($x$.mapped\_value)]} //sort x based on mapped values \\ 
     %\texttt{sorted_indexes = x_sorted.indexes()}
     %\texttt{x = x[sorted_indexes]}
     %\texttt{Divide x into equal size pages according to num\_of\_cells}\\
     %\texttt{Store mapped value of first and last key for each page }\\
      \For{$i\gets0$ \KwTo N}
     {
         \texttt{cell[$i$].lower = $x$[$i$* $K$].mapped\_value } \\
		\texttt{cell[$i$].upper = $x$[($i$+1)*$K$].mapped\_value } \\  
		
     }
     \Return \texttt{cell}
		   
     \caption{Training Algorithm for Lisa Baseline Method}
     \label{algo:Training_Lisa_Baseline}
\end{algorithm}

In the Algo. \ref{algo:Training_Lisa_Baseline},  training of LISA baseline model is described in the following steps:

\begin{enumerate}
	\item $N$, which represents the number of cells into which the key's mapped value space will be divided.
    \item In lines $1$ to $3$, we calculate the mapped value of each item in the training set.
	\item On line $4$, we calculate the number of keys per cell. 
	\item On line $5$, Sort train set according to keys' mapped values.
	 \item In lines $6$ to $9$, we divide the keys into equal sized cells. Per cell we need to store meta data for two keys, corresponding to first and last key in the cell 
\end{enumerate}



For prediction, we find the cell corresponding to mapped value of the query point using binary search, scan this cell sequentially and compare the values of keys in the cell against the query point, until a match is found.

\subsubsection{Prediction}

\begin{algorithm}[H]
    \SetAlgoLined
    \SetKwInOut{Input}{input}
    \SetKwInOut{Output}{Output}
     \Input{x\_test : query\_point; cell : cell\_metadata\_array: $x$ : training\_database\_array}
     \Output{x\_test.value : query\_point\_value }
    %\Input{\texttt{$x\_test$: query_point}; \texttt{$d$:}Array with metadata for each cell}
    \texttt{cell\_found = False }\\
    \texttt{x\_test.mapped\_value = x\_test[0]+x\_test[1] } \\
    \For{$i\gets0$ \KwTo len(cell)}
    {
        \If{ x\_test.mapped\_value$\in$ [cell[$i$].lower, cell[$i$].upper)} 
        {
		    %\texttt{Key is in Page $i$ } \\
		    \texttt{cell\_found = True }\\
		    \texttt{break }
		   
		}
    }
    \uIf{cell\_found==True} {
    `   \texttt{$K$ =  cell.keys\_per\_page  }\\
        \texttt{cell\_offset = $K$*$i$  }\\
    	 \For{$i\gets cell\_offset$ \KwTo $K$+cell\_offset}
    {   
        \If{(x\_test[0] == $x$[$i$][0]) and (x\_test[1] == $x$[$i$][1])  } 
        {
		   	\Return \texttt{$x$[$i$].value}
		   
		}
    }
    }
  
 	 	\Return \texttt{-1}
     \caption{Prediction Algorithm for LISA Baseline Model }
\end{algorithm}


\subsection{LISA Overview}

Given a spatial dataset, we generate the mapping function $\mathcal{M}$ and the shard prediction function $\mathcal{SP}$. Based on them, we build our index structure, LISA, to process point, range and $K$NN queries. LISA consists of four parts: the representation of grid cells, the mapping function $\mathcal{M}$, the shard prediction function $\mathcal{SP}$, and the local models for all shards. As illustrated in the Fig \ref{fig:LISA_Framework}. the procedure of building LISA is composed of four parts.

\begin{figure*}[t]
    \centering
	\includegraphics[width=1\textwidth]{graphs/implementation/lisa_overview.pdf}
    \caption{LISA Framework. In this figure, 1) Generate grid cells, and apply Lebesgue Measure to each cell to map two dimensional key value to a scalar. 2) Sort mapped values and divide them across equal length intervals termed as mapped intervals (3 in our figure). 3) For each mapped interval, divide the mapped value range in shards  (3 in our figure) and learn a linear regression function to partition the keys belonging to a particular interval, into different shards }
    \label{fig:LISA_Framework}
\end{figure*}

\begin{enumerate}
	\item Grid cell partition.
	\item Mapping spatial coordinates into scalars, i.e. $\mathbb{R}^d\to\mathbb{R}$.
	\item Build shard prediction function $\mathcal{SP}$.
	\item Build local models.
\end{enumerate}

\subsubsection{Definitions}

This section presents the additional definition specific to LISA implementation.

\begin{enumerate}
\setcounter{enumi}{3}
	\item \textbf{Shard}. The shard $S$ is the pre-image of an
interval $[a, b) \subseteq [0, +1)$ under the mapping function $\mathcal{M}$,  i.e., $S = M^{-1}([a.b))$. \\
%TODO: move this paragraph into somewhere more suitable
Given an initial data set, we divide the key space into cell grids based on the data distribution, map keys values to an one dimensional space using mapping function, followed by learning several monotonic shard prediction functions. After sorting, the one dimensional mapped value space is divided into equal-length intervals. One shard prediction function is learned for each interval, to partition the keys belonging to a particular interval, into different shards. As keys are sorted by mapped values before partitioning them into equal sized intervals, and all shards exhibit a total order with respect to their corresponding intervals in the mapped range (Shard Prediction function for each interval is monotonically increasing), following relationship holds
$$ \text{inf} (M(S_{i}))  > \text{sup} (M(S_{j}))\: \text{if}\: i > j$$

\item \textbf{Local Model}. Local model $L_{i}$ is a model that processes operations within a shard $S_i$. It keeps dynamic structures such as the addresses of pages contained by $S_{i}$. Local models are not relevant to our implementation as full data-set is loaded in the main memory.
\end{enumerate}

\subsection{LISA Implementation in Details}
\subsubsection{Grid Cells Generation}
The first task in LISA implementation is to partition the $2$ dimensional key space into a series of grid cells based on the data distribution along a sequence of axes. Then we number the cells along these axes as well. The principal idea behind this partition strategy is to divide the key space into cell boundaries and apply a mapping function to create monotonically increasing mapping values at the cell boundaries. 

%TODO: change the principal idea behind.

    $$ M(x_{i} \in V) <  M(x_{j} \in V) \text{ for } x_{i} \in C_{i}, x_{j} \in C_{j}; \text{ if } i<j$$
    
The above inequality means that the mapped value of a key in cell $i$ will always be less than mapped values of a key in cell $j$, if $i <j$.

\begin{mscexample}
	Consider the example shown in the figure \ref{fig:Cell_Parttion}.
	\begin{enumerate}
		\item Plot A shows distribution of 27 keys in 2-D space. 
		\item In plot B, we first sort Keys on $1^{st}$ dimension and divide into 3 vertical columns each containing 9 keys. 
		\item Then for each vertical column of $9$ keys, we sort the keys again according to $2^{nd}$ dimension, and divide the keys in each vetical column into $3$ new cells. 
		\item The total number of cells into which the keys space is divided, is a hyper-parameter and found empirically using grid search.
	\end{enumerate}
\end{mscexample}

We need to sort the key space along the sequence of axis before we partition the keys value along that axis to make sure that cells don't contain overlapping keys.

\begin{figure*}[t]
    \centering
    \includegraphics[width=1\textwidth]{graphs/implementation/cell_generation.pdf}
    \caption{Cell Partition Strategy.}
    \label{fig:Cell_Parttion}
\end{figure*}

\begin{algorithm}[H]
    \SetAlgoLined
    \SetKwInOut{Input}{input}
      \Input{$N$ : length\_of\_grid\_cell; trainset : [$(x,y);x \in \mathbb{R}^{2};y \in \mathbb{R}$]}
     %\texttt{trainset=[$(x,y);x \in \mathbb{R}^{2};y \in \mathbb{R}$]} \\
     \texttt{$K$ =  len($x$)\slash$(N * N) $} // K: Keys per cell \\
      \texttt{$x$ = $x$[argsort($x$[0])]} //Sort x based on $1^{st}$ dimension \\
     \For{$i\gets0$ \KwTo $N$}
      {
         \For{$j\gets0$ \KwTo $N$}
         {
           cell[$i$+$j$*$N$].lower[0] = $x$[$i$ *$K$ *$N$][0] // Store keys's x coordinate for first key in cell.\\
           cell[$i$+$j$*$N$].upper[0] = $x$[($i+1$) *$K$ *$N$][0] // Store keys's x coordinate for last key in cell.\\
		 }
      }
     \For{$i\gets0$ \KwTo $N$}  
     {
        \texttt{$x$[$i$*$K$*$N$:($i+1$)*$K$*$N$] = $x$[argsort($x$[$i$*$K$*$N$:($i+1$)*$K$ *$N$])+$i$*$K$*$N$][1]} //Sort x based on $2^{nd}$ dimension \\
     } 
     \For{$i\gets0$ \KwTo $N$}
      {
         \For{$j\gets0$ \KwTo $N$}
         {
            cell[$i$+$j$*$N$].lower[1] = $x$[$j$ *$K$ *$N$][0] // Store keys's y coordinate for first key in cell.\\
            cell[$i$+$j$*$N$].upper[1] = $x$[($j+1$) *$K$ *$N$][0] // Store keys's y coordinate for last key in cell.\\
		 }
      }
     \caption{Grid Cell Generation Algorithm for LISA Method}
     \label{algo:grid_cell_generation}
\end{algorithm}

In the Algo. \ref{algo:grid_cell_generation},  segregation of key space into cells is performed in the following steps:

\begin{enumerate}
	\item Row length of the grid cell $N$ is passed as input. In our implementation grid cell is constrained to be a square and total number of cells is given by $N$*$N$.
	\item  Then we calculate number of keys per cell ($K$) and sort $x$ based on $1^{st}$ dimension.
	\item In lines $3$ to $8$, we divide the key space into $N$ vertical columns each containing $K$*$N$ keys. Since our cell grid is $N$*$N$, for each cell along x dimension, we store the same values of keys's x coordinates for $N$ cells along y dimension, thereby creating a vertical column of $N$ cells for each cell along x dimension. Per cell we need to store meta data for two keys, corresponding to first and last key in the cell. 
	
	\item In lines $9$ to $11$, for each vertical column of $K$*$N$ keys, we sort the keys again according to $2^{nd}$ dimension.
	\item In lines $12$ to $17$, we divide the keys in each vertical column into $N$ new cells and store the keys's y coordinates.
\end{enumerate}


\subsubsection{Mapping Function}
\label{sssec:Mapping_Function}
A mapping function $\mathcal{M}$ is a function on the domain $\mathbb{R}^{2}$ to the non-negative range, i.e $M:[0,X_{0}]\times[0,X_{1}]\to [0,+\infty)$ such that
    $ M(x_{i} \in V) <  M(x_{j} \in V)$ if $i<j$, where $x_{i} \in C_{i}$ and $x_{j} \in C_{j}$. That means the mapped value of a key in cell $i$ will always be less than mapped values of a key in cell $j$, if $i <j$. 
    
Suppose $x = (x_{0}, x_{1})$ and $x \in C_{i} = [\theta^{(0)}_{i_0},\theta^{(0)}_{i_0+1}) \times [\theta^{(1)}_{i_1},\theta^{(1)}_{i_1+1}) $ then we define 
$$M(x) = i+ \frac {\mu(H_{i})}{\mu(C_{i})} $$ where $H_{i} = [\theta^{(0)}_{i_0},x_{0}) \times [\theta^{(1)}_{i_1},x_{1}) $ and $\mu$ is the Lebesgue measure on $\mathbb{R}^2$.

As shown in figure \ref{fig:Lebesgue_Measure}, in $2$-dimensional case, $\frac {\mu(H_{i})}{\mu(C_{i})}$ represents the fraction of the area covered by the key$(x_{0}, x_{1})$ to the total area of the cell. Since we are adding $i$, the index of the cell, to this fraction, the mapped value of a key in cell $i$ will always be less than mapped values of a key in cell $j$, if $i<j$. After calculating the mapped values of the data set, we sort the keys in each cell according to the mapped value. This results in the whole key space to be sorted according to the mapped value. Figure \ref{fig:Mapped_Cdf} shows the mapping of $2$ dimensional key space to one dimensional CDF.

\begin{figure*}[t]
    \centering
    \includegraphics[width=0.6\textwidth]{graphs/implementation/Lebesgue_Measure.pdf}
    \caption{Lebesgue Measure Representation for 2 dimensional data.}
    \label{fig:Lebesgue_Measure}
\end{figure*}

\begin{mscexample}
	In \ref{fig:Lebesgue_Measure}, we calculate the Lebesgue measure for the black and green points as examples.
	\begin{enumerate}
		\item For the black point in the first cell, the Lebesgue measure will be ratio of area of red rectangle divided by the total area of $1^{st}$ cell, i.e. $50/100=0.5$.
		\item For the green point in the second cell, the Lebesgue measure will be ratio of area of blue rectangle divided by the total area of $2^{nd}$ cell, i.e. $20/100= 0.2$ 
	\end{enumerate}
\end{mscexample}

\begin{figure*}[t]
    \centering
    \includegraphics[width=1\textwidth]{graphs/Mapped_cdf.png}
    \caption{Mapping 2 dimensional key Values to one dimensional cdf.} 
    %In this figure, we first generate grid cells, and apply Lebesgue Measure to each cell in 1). Then in the 2) step, we sort key in each cell according to mapped value. Mapped values in consecutive cells are already sorted by mapping function definition. The third plot is the CDF of mapped values.}
    \label{fig:Mapped_Cdf}
\end{figure*}

\subsubsection{Shard Prediction Function}

After the mapping function, we get a dense array of mapped values. Then we partition them evenly into $U$ parts and let $\boldsymbol{M}_p=[m_1,\cdots, m_U]$. We train linear regression functions $\mathcal{F}_i$ on each interval and suppose $V+1$ is the number of mapped values that each $\mathcal{F}_i$ needs to process and $D$ is the number of shards per interval. $\Psi$ =$\floor{\frac{V+1}{D}}$ is the number of keys falling in a shard. %With these definitions, we know that each $\mathcal{F}_i$ handles  $D=\ceil{\frac{V+1}{\Psi}}$ shards.

\begin{mscexample}
	For example, assume we have a dense array of 9 mapped values as $$[1,1.2,2, 2.2,3,3.3,3.4,4, 4.5]$$
	
and $U$ and $D$ are initialized as 3. So we have $\boldsymbol{M}_p=[9]$ which is divided into 3 equal intervals, $\boldsymbol{M}_p=[m_1, m_2, m_3]$, each containing 3 keys. In this case we have $V+1 = 3$ and will train $3$ linear regression functions, 1 for each interval. Each $\mathcal{F}_i$ generates $D$ shards and number of keys falling in a shard will be $\Psi= \floor{\frac{V+1}{D}} = 1$.
\end{mscexample}


Then with a given $x$, the predicted shard is given by $\mathcal{SP}(x)=\mathcal{F}_i(x)+i\times D$, where $i=\text{binary-search}(\boldsymbol{M}_p,x)$. More specifically, we first determine $i$ by using binary search. The result tells which interval this $x$ should belong to. Then we find the corresponding linear regression function $\mathcal{F}_i$ and calculate $\mathcal{F}_i(x)$, which is the predicted shard.

\begin{mscexample}
	In the above example, given a key $x=1.2$, we first perform binary search in $\boldsymbol{M}_p$ and we found $i=1$. Then we find the first linear regression function $\mathcal{F}_1$ and calculate $\mathcal{F}_1(x)$. Since each linear regression function will yield $D=3$ shards, the shards that the first linear regression function generates will be from $0$ to $2$ and the shards that the second linear regression function generates will be from $3$ to $5$. Hence, the predicted shard id is given by 
$$
\mathcal{SP}(x)=\mathcal{F}_i(x)+i\times D
$$
\end{mscexample}

Then the problem left is to train the linear regression functions $\mathcal{F}_i$. Let $\boldsymbol{x}=(x_0,\cdots,x_v)$ be the keys' mapped value that fall in $[m_{i-1}, m_i)$. Suppose that $\boldsymbol{x}$ is sorted, i.e. $x_i\leq x_j, \forall 0\leq i<j\leq v$. Let $\boldsymbol{y}=(0,\cdots, V)$. Then we build a piecewise linear regression function $f_i$ with inputs $\boldsymbol{x}$ and ground truth $\boldsymbol{y}$. For a given point with mapped value $m\in[m_{i-1}, m_i)$, its shard id is given by $\ceil{\frac{f_i(m)}{\Psi}}+i\times D$, i.e. $\mathcal{F}_i(x)=\frac{f_i(m)}{\Psi}$.

\begin{mscexample}
	In our previous example, in the interval $[0,2]$, we have $\boldsymbol{x}=(1,1.2,2)$ and $\boldsymbol{y}=(0,1,2)$. Then for a point with the mapped value $m=1.2$, the expected output will be $f_i(m)=1$ and the shard id is given by $\ceil{\frac{1}{1}}+0\times 2=1$. Hence, the point with mapped value $m=1.2$ will be allocated to the second shard with shard id 1. Then the problem is to train a continuous piecewise linear regression function in each interval. We constrain the piecewise linear regression function to be continuous so that it is guaranteed be monotonic as shown in Figure \ref{shardPrediction}.
\end{mscexample}

\begin{figure*}[t]
    \centering
	\includegraphics[width=0.6\textwidth]{graphs/implementation/shardPrediction.pdf}
    \caption{Piecewise linear regression functions learnt by the piecewise linear function in the shard prediction training algorithm, $V+1$ is the number of keys per mapped interval.}
    \label{shardPrediction}
\end{figure*}

Formally, a piecewise linear function can be described as 

\begin{equation}
\label{piecewise_linear_function}
	f(x)= \begin{cases} 
      b_0+\alpha_0(x-\beta_0) & \beta_0\leq x < \beta_1 \\
      b_1+\alpha_1(x-\beta_1) &  \beta_1\leq x < \beta_2 \\
      \vdots \\
      b_\sigma+\alpha_\sigma(x-\beta_\sigma) &  \beta_\sigma\leq x \\
   \end{cases}
\end{equation}

In order to make this piecewise linear function continuous, the slopes and intercepts of each linear region depend on previous values. Formally, let $\bar{a}=b_0$, then Eq. (\ref{piecewise_linear_function}) reduces to

\begin{equation}
	\label{continuous_piecewise_linear_function}
	f(x)= \begin{cases} 
      \bar{\alpha}+\alpha_0(x-\beta_0) & \beta_0\leq x < \beta_1 \\
      \bar{\alpha}+\alpha_0(x-\beta_0) + \alpha_1(x-\beta_1) &  \beta_1\leq x < \beta_2 \\
      \cdots \\
      \bar{\alpha}+\alpha_0(x-\beta_0) + \alpha_1(x-\beta_1)+\cdots+\alpha_\sigma(x-\beta_\sigma) &  \beta_\sigma\leq x \\
   \end{cases}
\end{equation}


Then to make Eq. (\ref{continuous_piecewise_linear_function}) monotonically increasing, we only need to ensure that $$\sum_{i=0}^\eta \alpha_i\geq 0, \forall 0\leq \eta\leq \sigma$$

Let $\boldsymbol{\alpha}=(\bar{\alpha},\alpha_0,\cdots,\alpha_\sigma)$, the square loss function $L(\boldsymbol{\alpha},\boldsymbol{\beta})=\sum_{i=1}^{V}(f(x_i)-y_i)^2$. We then optimise $\boldsymbol{\alpha}$ and $\boldsymbol{\beta}$ iteratively.

Assume that $\boldsymbol{\beta}=\hat{\boldsymbol{\beta}}=(\hat{\beta_0},\hat{\beta_1},\cdots,\hat{\beta_\sigma})$ is fixed, then $\boldsymbol{\alpha}$ can be regarded as the least square solution of the linear equation $\boldsymbol{A\alpha}=\boldsymbol{y}$, where

$$
\boldsymbol{A}=\left[\begin{array}{ccccc}
1 & x_{0}-\hat{\beta}_{0} & \left(x_{0}-\hat{\beta}_{1}\right) 1_{x_{0} \geq \hat{\beta}_{1}} & \ldots & \left(x_{0}-\hat{\beta}_{\sigma}\right) 1_{x_{0} \geq \hat{\beta}_{\sigma}} \\
1 & x_{1}-\hat{\beta}_{0} & \left(x_{1}-\hat{\beta}_{1}\right) 1_{x_{1} \geq \hat{\beta}_{1}} & \ldots & \left(x_{1}-\hat{\beta}_{\sigma}\right) 1_{x_{1} \geq \hat{\beta}_{\sigma}} \\
\vdots & \vdots & \vdots & \ddots & \vdots \\
1 & x_{V}-\hat{\beta}_{0} & \left(x_{V}-\hat{\beta}_{1}\right) 1_{x_{V} \geq \hat{\beta}_{2}} & \cdots & \left(x_{V}-\hat{\beta}_{\sigma}\right) 1_{x_{V} \geq \hat{\beta}_{\sigma}}
\end{array}\right]$$

where $1_{x_{0} \geq \hat{\beta}_{1}}$ equals to $1$ if ${x_{0} \geq \hat{\beta}_{1}}$, otherwise it equals to $0$.

We have
\begin{equation}
 \begin{split}
	L(\boldsymbol{\alpha},\boldsymbol{\beta}) 
	 =(\boldsymbol{y-A\alpha})^T(\boldsymbol{y-A\alpha}) 
	&=\boldsymbol{y}^T\boldsymbol{y}-\boldsymbol{\alpha}{^T}\boldsymbol{A}^T\boldsymbol{y}-\boldsymbol{y}^T\boldsymbol{A\alpha}+\boldsymbol{\alpha}^T\boldsymbol{A}^T\boldsymbol{A\alpha} \\
	& = \boldsymbol{y}^T\boldsymbol{y}-2\boldsymbol{\alpha}^T\boldsymbol{A}^T\boldsymbol{y}+\boldsymbol{\alpha}^T\boldsymbol{A}^T\boldsymbol{A}\boldsymbol{\alpha}
\end{split}
\end{equation}

and if we let 

\begin{equation}
\label{alpha_form}
	\begin{split}
		\frac{\partial L(\boldsymbol{\alpha}, \boldsymbol{\beta})}{\boldsymbol{\alpha}}=2\boldsymbol{A}^T\boldsymbol{A}\boldsymbol{\alpha}-2\boldsymbol{A^T}\boldsymbol{y}=0 \\ \implies 
		\boldsymbol{\alpha}=(\boldsymbol{A}^T\boldsymbol{A})^{-1}\boldsymbol{A}\boldsymbol{y}
	\end{split}
\end{equation}


we get the $\boldsymbol{\alpha}$ with the given fixed $\boldsymbol{\beta}$. Clearly, different $\boldsymbol{\beta}$ give rise to different optimal parameters. Let $\boldsymbol{\alpha^\star}(\boldsymbol{\beta})$ be the optimal $\boldsymbol{\alpha}$ for a particular $\boldsymbol{\beta}$, then we want to find $\boldsymbol{\beta}$ such that


\begin{equation}
	L(\boldsymbol{\alpha^\star}(\boldsymbol{\beta^\star)}, \boldsymbol{\beta^\star})=\text{min}\{L(\boldsymbol{\alpha^\star}(\boldsymbol{\beta)}, \boldsymbol{\beta}) | \boldsymbol{\beta\in\mathbb{R}^{\sigma+1}}\}
\end{equation}

For $\boldsymbol{\beta}$, we define $\boldsymbol{r}=\boldsymbol{A\alpha-y}$ and 

$$
\boldsymbol{K}=\text{diag}(\bar{\alpha},\alpha_0, \cdots, \alpha_\sigma), \boldsymbol{G}=\begin{bmatrix}
 -1 & -1 & \cdots & -1 \\
  p_0^{(0)} & p_0^{(1)} & \cdots & p_0^{(V)} \\
  p_1^{(0)} & p_1^{(1)} & \cdots & p_1^{(V)} \\
  \vdots & \vdots & \ddots & \vdots \\
  p_\sigma^{(0)} & p_\sigma^{(1)}& \cdots & p_\sigma^{(V)} \\
\end{bmatrix}
$$

where $p_i^{(l)}=-1_{x_l\geq \beta_i}$. Then

$$
\boldsymbol{KG}=\begin{bmatrix}
 -\bar{\alpha} & -\bar{\alpha} & \cdots & -\bar{\alpha} \\
 0 & \alpha_0p_0^{(1)} & \cdots  & 0 \\
 \vdots & \vdots & \ddots & \vdots \\
 0 & 0 & \cdots & \alpha_\sigma p_\sigma^{(V)}
\end{bmatrix}
$$

then we have 

\begin{equation}
	g=\frac{\partial L(\boldsymbol{\alpha},\boldsymbol{\beta})}{\partial \boldsymbol{\beta}}=2\boldsymbol{KGr},
	Y=\frac{\partial g}{\partial \boldsymbol{\beta}}=2\boldsymbol{KGG}^T \boldsymbol{K}^T
\end{equation}

As $g=\nabla_{\boldsymbol{\beta}} L$, $-g$ specifies the steepest descent direction of $\boldsymbol{\beta}$ for $L$. However, the convergence rate of $-g$ is low as it does not consider the second order derivative of $L$. Hence, we use Newton's method to perform the update along the direction of second derivative, $s=-\boldsymbol{Y}^{-1}g$. Newton's method assumes that the loss L is twice differentiable and uses the approximation with Hessian
The geometric interpretation of Newton's method is that at each iteration, it amounts to the fitting of a paraboloid to the surface of $L(\boldsymbol{\alpha},\boldsymbol{\beta})$  at the trial value $\beta_{k}$, having the same slopes and curvature as the surface at that point, and then proceeding to the maximum or minimum of that paraboloid. 
Hessian matrix, Y in our case is positive semidefinite and hence can be inverted. 
\begin{equation}
	Y=\frac{\partial g}{\partial \boldsymbol{\beta}}=2\boldsymbol{KGG}^T \boldsymbol{K}^T= 2 \boldsymbol{(KG)} (\boldsymbol{G}^T \boldsymbol{K}^T)=2 (\boldsymbol{G}^T \boldsymbol{K}^T)^T (\boldsymbol{G}^T \boldsymbol{K}^T)= 2\boldsymbol{({M}^TM)}
\end{equation}

Y is a full rank matrix as columns of Y are linearly independent (all keys are independent of each other). To prove that Y is positive definite, we need to show that ${x}^TYx > 0,  \forall x \neq 0$. \\ 
${x}^TYx = {x}^T{M}^TMx = {(Mx)}^T(Mx) = \| Mx\|_{2}^{2} \geq 0,\forall x \neq 0 $

In the beginning, we set $\beta^{(0)}=x_0$ and $\beta_i^{(0)}=x_{\floor{i\times \frac{V}{\Psi}}}, \forall i\in[1,\sigma]$. Then we can obtain $\boldsymbol{\alpha}$ by solving Eq. (\ref{alpha_form}). Then at each step, we perform a grid search to find the step $lr^{(k)}$ such that the loss $L$ is minimal. Then at the next iteration, we increase $k$ by one and set 

$$
\boldsymbol{\beta}^{(k+1)}=\boldsymbol{\beta}^{(k)} + lr^{(k)}s^{(k)}
$$

As described in Algorithm \ref{Shard_Training_LISA}, we perform  following operations during shard training, :

\begin{enumerate}
	\item Divide the sorted mapped values into equal sized $U$ intervals. We found empirically that training algorithm generalizes better if mapped intervals are aligned with grid cell boundaries. $U$ is initialized to numbers of grid cells. 
	\item Suppose $V +1$ is the number of mapped values in each interval and $D$ is the  number of shards learned per mapped interval.
	\item For each interval, we want to build a monotonic regression
    model $ \mathcal {F}_{i}$ whose domain is $[m_{i-1},m_{i}]$ 
 
 	\item Each $\mathcal{F}_{i}$ generates $D$ shards and every such shard contains $\Psi = \floor{\frac{V+1}{D}}$ number of keys 
    
    \item $x =[x_0,\cdots, x_V] $ specifies the keys' mapped values in interval i, $[m_{i-1},m_{i}]$ 
    
    \item Given $V +1$ sorted mapped values $x =[x_0,\cdots, x_V]$ and their indices $y =[0,\cdots, V]$, each $\mathcal{F}_{i}$ is built and trained with the procedure mentioned in the algorithm \ref{Shard_Training_LISA}.
    

\end{enumerate}

\begin{algorithm}[H]
    \SetAlgoLined
    \SetKwInOut{Input}{input}
    \Input{\texttt{$M_{p}$}:sorted mapped value array,U: number of mapped intervals, $D$:number of shards per interval}
    \texttt{Partition $M_{p}$ into equal length U intervals $\boldsymbol{M}_p=[m_1,\cdots, m_U]$ }
    \For{$i\gets0$ \KwTo $U$}
    {   
        \texttt{$x =[x_0,\cdots, x_V] $ be the keys' mapped values
        in interval $i$} \\
        \texttt{$y =[0,\cdots, V]$ } \\
        \texttt{Initialize $\beta^{(0)}$ as $\beta^{(0)}=x_0$ and $\beta_i^{(0)}=x_{\floor{i\times D}}, \forall i\in[1,\sigma]$} \\
        \While{\texttt{k} \textless \texttt{iter}} 
        {
            \texttt{Initialize $A^{(k)}$ according to \eqref{continuous_piecewise_linear_function}} \\
            \texttt{$\alpha^{(k)}= ((A^{(k)})^T A^{(k)})^{-1}(A^{(k)})^Ty$} \\
            
            \texttt{Calculate $g^{(k)}, Y^{(k)}$ } \\
            \texttt{$s^{k} = -(Y^{(k)})^{-1} g^{(k)}, $ } \\
            
            \texttt{Find update step $lr^{(k)}$ such that    L$(\alpha^\star(\beta^{k}+ lr^{(k)}s^{k}), \beta^{k}+ lr^{(k)}s^{k}) =\text{min}\{L(\alpha^\star(\beta^{k}+ lr^{(k)}s^{k}), \beta^{k}+ lr^{(k)}s^{k})\} $} \\
            
            \texttt{$\beta^{k+1} = \beta^{k}+ lr^{(k)}s^{k}$ } \\
        }
    }
    
    \caption{Shard Training Algorithm}
    \label{Shard_Training_LISA}
\end{algorithm}

\subsubsection{Local Models for Shards}
Local models are not relevant to our implementation as full data-set is loaded in the main memory.  
