For two dimensional data, the evaluation covers the following tasks:

\begin{itemize}
 	\item Find hyper-parameters for the LISA Baseline model empirically.
	\item Find hyper-parameters for the LISA model empirically.
	\item Compares the performance between $K$D-tree, LISA Baseline and LISA models for the point query.
	\item Compare the performance between $K$D-tree, LISA Baseline and LISA models for the range query.
	\item Compare the performance between $K$D-tree and LISA models for KNN query. $K$NN Query has not been implemented for LISA Baseline as there is no description of $K$NN Query for Baseline model in the paper. 
\end{itemize}

\subsection{Dataset}

For two dimensional case, we manually generate three columns of the data:

\begin{itemize}
	\item The first two columns contain the  2-dimensional keys $\boldsymbol{X} \in \mathbb{R}^{2}$, which are independently sampled from a lognormal Distribution. The dataset contains 190 million key-value pairs.
	\item Then we assign the keys into different pages according to a preset parameter $N_{page}$ for page size. Specifically, the first $N_{page}$ keys will be assigned to the first page, the second $N_{page}$ keys will be assigned into the second page and so on so forth. After the assignments, we set the second column $Y$ to be the page index of the corresponding $x$.
\end{itemize}

Our final data-set consists of 190 million key-value pairs that are distributed under lognormal distribution.

As discussed in previous section, there are multiple challenges in using the complete dataset for training and hyper-parameters tuning. Even on google cloud server, running experiments with the full data take considerable long times (Lisa model took 26 hours to build), we had limited cloud server budget and a large number of experiments to run. Therefore, for two dimensional indexes evaluation, we have used sampling to generate smaller training datasets.   

\subsection{Hyper-parameters Search}

After generating dataset as mentioned in previous section, we sample a smaller subset from it. We repeat our experiments for $3$ different sample sizes of 10000, 100000 and 1000000 points. Test data is a copy of training data for all our experiments. For Baseline and Lisa models, final prediction is given by linear search through a range of values (identified as a Cell for Baseline and Shard for LISA model) and mean square error (MSE) is zero as test points are already learned during training. This is where Learned Index models differ from traditional machine learning models where model performance is evaluated on unseen data. 

\subsubsection {Hyper-parameter search for the LISA baseline}
Baseline model has one hyper-parameter: N (Number of cells specifying the number of equal length intervals into which mapped values are divided). The point query search consists of two parts, first is binary search to locate the cell into which the query key is located, followed by sequentially comparison of the query key value with keys in the found cell until a match is found. The time complexity of first search is $log_{2}N_{1}$, where $N_{1}$ is the number of cells. The time complexity of second search is  $ \left \lceil {N_{2} / 2}\right \rceil $, where $N_{2}$ is the number of keys per cell.  

\begin{figure}
 \centering
     \begin{subfigure}[b]{0.32\textwidth}
         \centering
         \input{graphs/evaluation/two-d/task3_1_1.tex}
         \caption{Average Query Time (ms)}
         \label{fig:2d_exp3_1_1}
     \end{subfigure}
     \hfill
     \begin{subfigure}[b]{0.32\textwidth}
         \centering
         \input{graphs/evaluation/two-d/task3_1_2.tex}
         \caption{Memory Size (KB)}
         \label{fig:2d_exp3_1_2}
     \end{subfigure}
     \hfill
     \begin{subfigure}[b]{0.32\textwidth}
         \centering
         \input{graphs/evaluation/two-d/task3_1_3.tex}
         \caption{Build Time (s)}
         \label{fig:2d_exp3_1_3}
     \end{subfigure}
     \hfill
     \caption{Hyper-parameter search in LISA Baseline for training sizes $10K$, $100K$ and $1M$.}
     \label{fig:LISA_Baseline_Hyperparameter_Search}
\end{figure}


\begin{mscconclusion}
	Following conclusions can be drawn from experimental results shown in table \ref{small_lognormal_lisa_baseline_10000} and Fig. \ref{fig:LISA_Baseline_Hyperparameter_Search}
\begin{enumerate}
    \item Optimum value of hyper-parameter N will be equal to number of points in the training data-set, resulting in 1 key per cell and search query time of O($log_{2}N$).
	
	\item Average Query Time:  Average Query Time decreases with increase in value of N as number of keys per cell decreases.
	\item Build time: Build time increases with increase in value of N, as metadata for additional cells needs to be calculated. 
	\item Memory Size:  Memory requirements of the model increases with increase in value of N, as metadata for additional cells needs to be stored. Increase  in memory size is not significant with increase in N as we maintain only two values per cell, mapped value of first key in the cell and mapped value of last key in the cell.
\end{enumerate}
\end{mscconclusion}

\subsubsection {Hyper-parameter search for the LISA implementation}
For LISA model, we have 3 hyper parameters:
\begin{enumerate}
	\item $G$: The size of the grid cell. Number of grid cells into which the key space is divided. In our implementation, we use a square grid cell, and total number of cells is given by $G$ $\times G$.
	\item $N$: Number of equal length intervals into which mapped value range is divided. During our experiments, we found that shard prediction algorithm gives better performance if mapped interval boundaries are aligned to grid cell boundaries. That's why this parameter is always initialised to $N$=$G$ $\times G$   
	\item $S$: Number of shards to learn per mapped interval. 
\end{enumerate}

\begin{mscconclusion}
	Following conclusions can be drawn from experiments results shown in tables \ref{small_lognormal_lisa_10000}, \ref{small_lognormal_lisa_100000} and \ref{small_lognormal_lisa_1000000}. 
\begin{enumerate}

    \item For a particular value of $G$, average query time decreases and memory size increases with increase in value of S. This is expected as increasing S, will result in lesser number of keys per shard, thereby reducing the sequential search cost of scanning the query key through the Shard. 
    
    \item Average query time decreases and memory size increases with increase in values of $G$ and $S$. 
	
	\item We found emprically that value of $S$ should be choosen such that there are at least 45 keys per shard. We see mean square errors(mse) if number of keys per shard are less than 45 for following reasons. 
	\begin{enumerate}
	    \item For point query search, we first predict a shard and then sequentially compare the query point key values with all the keys in the predicted shard until a match is found
		\item For query points near the shard boundaries, there can be a mismatch in ground truth shard-id and predicted shard-id.If the query point is not found in the predicted shard, we continue our search in adjacent left and right shards in an empirically found range.
	\end{enumerate}
	
	During test experiments, we found that if shard size is less than 45 keys, sometimes shard prediction error can be greater than 1 and point query search can fail resulting in MSE errors.  
\end{enumerate}
\end{mscconclusion}

\subsection{Comparisons Across Models}

During following experiments, for each training data size, we have used hyper-parameters optimized for that particular data set size. 

\subsubsection{Point Query Comparison}
Table \ref{Point_Query_Comparision} and  Fig. \ref{fig:Point_Query_Comparision} shows the performance evaluation for $K$D-Tree, LISA-Baseline and LISA Models for different training data sizes. For a given training set, we perform point query evaluation for every point in the data-set and take the average. 

\begin{figure}
 	 \centering
     \begin{subfigure}[b]{0.32\textwidth}
         \centering
         \input{graphs/evaluation/two-d/task2_1_1.tex}
         \caption{Average Query Time (ms)}
         \label{fig:2d_exp2_1_1}
     \end{subfigure}
     \hfill
     \begin{subfigure}[b]{0.32\textwidth}
         \centering
         \input{graphs/evaluation/two-d/task2_1_2.tex}
         \caption{Memory Size (KB)}
         \label{fig:2d_exp2_1_2}
     \end{subfigure}
     \hfill
     \begin{subfigure}[b]{0.32\textwidth}
         \centering
         \input{graphs/evaluation/two-d/task2_1_3.tex}
         \caption{Build Time (s)}
         \label{fig:2d_exp2_1_3}
     \end{subfigure}
     \hfill
     \caption{Point Query experimental results for $K$D-Tree, Baseline and LISA models.}
     \label{fig:Point_Query_Comparision}
\end{figure}

\begin{mscconclusion}
The following conclusions can be concluded:

	\begin{enumerate}
    \item LISA outperforms $K$D-tree in terms of average query time. Search complexity of $K$D-Tree and LISA baseline( configured to keep 1 key per cell) is $O(N)$, and O($log_{2}N$) respectively where N is the number of points in the training data-set. On the other hand point query search cost in LISA is a combination of 4 costs.
    \begin{enumerate}
    \item Search cost to find the grid cell to which point query belongs. This cost increase linearly with increase in number of grid cells. 
    %Once the grid cell is found, calculate the mapped value as specified in the section.
    \item Search cost to find the mapped interval to which point query belongs. Search complexity of this cost is O($log_{2}U$), where U is the number of intervals into which sorted mapped array is divided. 
    
    \item Find the shard-id to which point query belongs. This cost is constant as shard prediction function weights are already learned during the build process. 
    
    \item Once the shard Id is found, search sequentially in the shard interval by comparing query point key value with all the keys in the shard until a match is found. This cost is relatively constant with respect to increase in training data-size as we try to initialize our hyper-parameters in such a way that number of keys per shard remain close to 50. 
\end{enumerate}
    
    \item LISA outperforms $K$D-tree in terms of memory size requirements. The storage consumption of LISA is considerably smaller than $K$D-Tree that has to construct a tree with all nodes and entries based on MBRs (minimum bounding rectangle)  and parent-children relationships. In contrast, LISA only keeps the parameters of $M$ and $SP$. Specifically,$M$’s parameters contain several numbers and a small list only, and $SP$ is composed of a series of piecewise linear functions whose parameters are a number of coefficients.
    
    \item LISA's build time is significantly higher than $K$D-Tree and LISA Baseline. The higher build time is caused by Shard Training Algorithm.  
\end{enumerate}

\end{mscconclusion}

\subsubsection {Range Query Experiments}
Table \ref{Range_Query_Experimental_Results} shows evaluation results for LISA,Baseline and $K$D-tree models for range sizes of 10, 100, 1000 for different training sizes. For a given range query size, we perform 20 trials and take the average. For each trial, we sample a random point from the test set and find the range from sampled point to the range query size. Average query time for each range is further divided by the range size to compare the query time across various ranges. As shown in the Fig. \ref{fig:Range_Query_Comparision}, LISA outperforms $K$D-tree for range query size of 10000 for all training sizes, however its range query time for smaller range sizes is significantly higher than $K$D-Tree.

\begin{figure}
 \centering
     \begin{subfigure}[b]{0.45\textwidth}
         \centering
         \input{graphs/evaluation/two-d/task2_2_1.tex}
         \caption{Range Query Size}
         \label{fig:2d_exp2_2_1}
     \end{subfigure}
     \hfill
     \begin{subfigure}[b]{0.45\textwidth}
         \centering
         \input{graphs/evaluation/two-d/task2_2_2.tex}
         \caption{Training Size}
         \label{fig:2d_exp2_2_2}
     \end{subfigure}
     \hfill
     \caption{Range Query experimental results for $K$D-Tree, Baseline and LISA models}
        \label{fig:Range_Query_Comparision}
\end{figure}

\begin{figure}
 \centering
     \begin{subfigure}[h]{0.45\textwidth}
         \centering
         \input{graphs/evaluation/two-d/task2_3_1.tex}
         \caption{Number of Nearest Neighbours ($K$)}
         \label{fig:2d_exp2_3_1}
     \end{subfigure}
     \begin{subfigure}[h]{0.45\textwidth}
         \centering
         \input{graphs/evaluation/two-d/task2_3_2.tex}
         \caption{Training Size (Fix $K=10$)}
         \label{fig:2d_exp2_3_2}
     \end{subfigure}
     \caption{$K$NN Query experimental results for $K$D-Tree and LISA models}
     %Lisa outperforms $K$D-Tree for all training data sizes. }
        \label{fig:KNN_Query_Comparision}
\end{figure}

\subsubsection {$K$NN Query Experiments}
Table \ref{KNN_Query_Experimental_Results} shows evaluation results for LISA and $K$D-tree models for $K$NN Queries for various value of K and training sizes. For a given $K$ value, we perform 20 trials and take the average of query time. For each trial, we sample a random point from the test set and find K neighbours around that point. Average query Time is further divided by $K$ to compare the Query time across various values of $K$.As shown in the Fig. \ref{fig:KNN_Query_Comparision}, LISA outperforms $K$D-tree for different training sizes and K values.


