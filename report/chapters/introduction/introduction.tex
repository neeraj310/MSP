Over the years, indexes have been widely used in databases to improve the speed of data retrieval. In the past decades, the database indexes generally fall into the hand-engineered data structures, such as B-Tree, KD-Tree, etc. These indexes have played a crucial role in databases and have been used widely in modern data management systems (DBMS) such as PostgreSQL. Despite their huge success, a shortcoming of these data structures is the lack of consideration of how the database records distributed. We use an example to demonstrate how distributions can affect the efficiency of database indexes.

\begin{mscexample}
	For example, if the dataset contains integers from $1$ to $1$ million, then the keys can be used directly as offsets. With the keys used as offsets, the value with a given key can be retrieved in $\mathcal{O}(1)$ time complexity while B-Tree requires $\mathcal{O}(\log n)$ time complexity for the same query. From the perspective of space complexity, we do not need any extra overhead by using the key as an offset directly, while the B-Tree needs extra $\mathcal{O}(n)$ space complexity to save the tree.
\end{mscexample}

From the above example, we found that there are two promising advantages of leveraging the distribution of the data:
\begin{enumerate}
  \item It may be faster when performing queries, especially when the number of entries in the database are rather huge.
  \item It may take less memory space, as we only need to save the model with constant size.
  \end{enumerate}

Nowadays, to learn the distribution and apply it to database indexes, Tim Kraska et al. proposed learned indexes \cite{kraska2018case}, \cite{li2020lisa}, where machine learning techniques are applied to automatically learn the distribution of the database entries and build the data-driven indexes. In this project, we implemented both hand-engineered indexes and the learned index. After that, we explore the possibilities of using convolutional neural networks as database indexes. This report is organised into the following chapters:

\begin{enumerate}
	\item \textbf{Introduction}. In this chapter, we illustrate the organisation of this report and introduce the general information about database indexes.
	\item \textbf{Implementation}. In this chapter, we present our implementation of the B-Tree for 1-D data and $K$D-Tree for 2-D data. We also present our implementation of the learned index for 1-D data \cite{kraska2018case} and 2-D data \cite{li2020lisa}. This includes a baseline learned index, a recursive model and all components of LISA framework. 
	\item \textbf{Evaluation}. In this chapter, we report evaluation of our implementation. 
	\item \textbf{Insights and Findings}. We demonstrate our findings in this chapter. Besides, we also discuss the advantages and disadvantages of different indexes.
	\item \textbf{Convolution and CNN for Learned Indexes}. In this chapter we explore the possibilities of using convolution operation and convolutional neural network to build learned indexes.
	\item \textbf{Conclusions}. 
\end{enumerate}

\section{Notations}

\input{chapters/introduction/notations}

\section{Terminologies}

\input{chapters/introduction/terms}

\section{Assumptions}

\input{chapters/introduction/assumptions}


